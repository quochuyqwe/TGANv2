{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wonqnaQMXHgU",
        "outputId": "7238f04a-0930-4361-ea13-2c9052728e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from scikit-video) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.11.2)\n",
            "Installing collected packages: scikit-video\n",
            "Successfully installed scikit-video-1.1.11\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iaViT5KyoyC4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class OptimizedDiscBlock(nn.Module):\n",
        "    def __init__(self, cin, cout):\n",
        "        \"\"\"Optimized Discriminator Block\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            cout (int): out channels\n",
        "        \"\"\"\n",
        "        super(OptimizedDiscBlock, self).__init__()\n",
        "        self.c1 = nn.Conv3d(cin, cout, kernel_size=3, padding=1)\n",
        "        self.c2 = nn.Conv3d(cout, cout, kernel_size=3, padding=1)\n",
        "        self.c_sc = nn.Conv3d(cin, cout, kernel_size=1, padding=0)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.avgp2d = nn.AvgPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # init\n",
        "        nn.init.xavier_uniform_(self.c1.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.c2.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.c_sc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.c1(x)\n",
        "        h = self.relu(h)\n",
        "        h = self.c2(h)\n",
        "        h = self.avgp2d(h)\n",
        "\n",
        "        s = self.avgp2d(x)\n",
        "        s = self.c_sc(s)\n",
        "\n",
        "        return h + s\n",
        "\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, cin, cout):\n",
        "        \"\"\"Discriminator Block\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            cout (int): out channels\n",
        "        \"\"\"\n",
        "        super(DisBlock, self).__init__()\n",
        "        self.c1 = nn.Conv3d(cin, cin, kernel_size=3, padding=1)\n",
        "        self.c2 = nn.Conv3d(cin, cout, kernel_size=3, padding=1)\n",
        "        self.s_sc = nn.Conv3d(cin, cout, kernel_size=1, padding=0)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # init\n",
        "        nn.init.xavier_uniform_(self.c1.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.c2.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.s_sc.weight)\n",
        "\n",
        "    def downsample(self, x):\n",
        "        ksize = [(2 if 1 < k else 1) for k in x.shape[2:]]\n",
        "        pad = [(0 if k % 2 == 0 else 1) for k in x.shape[2:]][::-1]\n",
        "        padf = []\n",
        "        for p in pad:\n",
        "            padf.append(p)\n",
        "            padf.append(p)\n",
        "        x = F.pad(x, padf)\n",
        "        return F.avg_pool3d(x, kernel_size=ksize, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.relu(x)\n",
        "        h = self.c1(h)\n",
        "        h = self.relu(h)\n",
        "        h = self.c2(h)\n",
        "        h = self.downsample(h)\n",
        "\n",
        "        s = self.s_sc(x)\n",
        "        s = self.downsample(s)\n",
        "        return h + s\n",
        "\n",
        "\n",
        "class DisResNet(nn.Module):\n",
        "    def __init__(self, channels=[64, 128, 256, 512, 1024], colors=3):\n",
        "        \"\"\"Singular discriminator with multiple DisBlocks\n",
        "\n",
        "        Args:\n",
        "            channels (list, optional): Defaults to [64, 128, 256, 512, 1024].\n",
        "            colors (int, optional): Color channels. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(DisResNet, self).__init__()\n",
        "        self.convs = nn.Sequential()\n",
        "        self.colors = colors\n",
        "\n",
        "        for i in range(len(channels)):\n",
        "            if not i:\n",
        "                self.convs.add_module(\n",
        "                    'OptDisc',\n",
        "                    OptimizedDiscBlock(colors, channels[0])\n",
        "                )\n",
        "            else:\n",
        "                self.convs.add_module(\n",
        "                    f'Down{i}',\n",
        "                    DisBlock(channels[i-1], channels[i])\n",
        "                )\n",
        "\n",
        "        self.fc = nn.Linear(channels[-1], 1)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[2] == self.colors:\n",
        "            x = x.transpose(1, 2)\n",
        "        h = self.convs(x)\n",
        "        h = torch.sum(h, dim=(2, 3, 4))\n",
        "        h = self.fc(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class DisMultiResNet(nn.Module):\n",
        "    def __init__(self, layers=4, channels=[64, 128, 256, 512, 1024], colors=3):\n",
        "        \"\"\"Multiple Discriminators to run inference on\n",
        "\n",
        "        Args:\n",
        "            layers (int, optional): Discriminator Count. Defaults to 4.\n",
        "            channels (list, optional): Defaults to [64, 128, 256, 512, 1024].\n",
        "            colors (int, optional): Color channels. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(DisMultiResNet, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.res = nn.ModuleList(\n",
        "            [DisResNet(channels, colors) for _ in range(layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert self.layers == len(x)\n",
        "        out = [self.res[i](x[i]) for i in range(self.layers)]\n",
        "        out = torch.cat(out, dim=0)\n",
        "        # out = sum(out)\n",
        "        # out = torch.sigmoid(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vqg-3f9UVtXv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "class CLSTM_cell(nn.Module):\n",
        "    def __init__(self, n_filters):\n",
        "        \"\"\"Convolutional LSTM Cell\n",
        "\n",
        "        Args:\n",
        "            n_filters (int): Number of LSTM channels\n",
        "        \"\"\"\n",
        "        super(CLSTM_cell, self).__init__()\n",
        "        self.w_x = nn.Conv2d(n_filters, n_filters * 4, kernel_size=3,\n",
        "                             padding=1)\n",
        "        self.w_h = nn.Conv2d(n_filters, n_filters * 4, kernel_size=3,\n",
        "                             padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x, h=None, c=None):\n",
        "        xifoc = self.w_x(x)\n",
        "        xi, xf, xo, xc = xifoc.chunk(4, dim=1)\n",
        "        if h is not None:\n",
        "            hi, hf, ho, hc = self.w_h(h).chunk(4, dim=1)\n",
        "        else:\n",
        "            hi, hf, ho, hc = torch.zeros_like(xifoc).chunk(4, dim=1)\n",
        "\n",
        "        if c is None:\n",
        "            c = torch.zeros_like(x)\n",
        "\n",
        "        ci = torch.sigmoid(xi + hi)\n",
        "        cf = torch.sigmoid(xf + hf)\n",
        "        co = torch.sigmoid(xo + ho)\n",
        "        cc = cf * c + ci * torch.tanh(xc + hc)\n",
        "        ch = torch.tanh(cc) * co\n",
        "\n",
        "        return ch, cc\n",
        "\n",
        "\n",
        "class CLSTM(nn.Module):\n",
        "    def __init__(self, n_filters, n_frames):\n",
        "        \"\"\"Full Convolutional LSTM\n",
        "\n",
        "        Args:\n",
        "            n_filters (int): Number of LSTM channels\n",
        "            n_frames (int): Frames to generate\n",
        "        \"\"\"\n",
        "        super(CLSTM, self).__init__()\n",
        "        self.cell = CLSTM_cell(n_filters)\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Assume z is in proper convolutional shape\n",
        "        out = torch.stack([torch.zeros_like(z)]*self.n_frames, dim=1)\n",
        "\n",
        "        h, c = None, None\n",
        "        for i in range(self.n_frames):\n",
        "            h, c = self.cell(z, h, c)\n",
        "            out[:, i] = h\n",
        "            z = torch.zeros_like(z)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, cin, cout):\n",
        "        \"\"\"Upscale and convolutions in ResNet setup\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            cout (int): out channels\n",
        "        \"\"\"\n",
        "        super(Up, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # define main branch\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.bn1 = nn.BatchNorm2d(cin)\n",
        "        self.convm1 = nn.Conv2d(cin, cout, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(cout)\n",
        "        self.convm2 = nn.Conv2d(cout, cout, kernel_size=3, padding=1)\n",
        "\n",
        "        # define skip branch\n",
        "        self.sconv = nn.Conv2d(cin, cout, kernel_size=1)\n",
        "\n",
        "        # initialize\n",
        "        nn.init.xavier_uniform_(self.convm1.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.convm2.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.sconv.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute main\n",
        "        h = self.bn1(x)\n",
        "        h = self.relu(h)\n",
        "        h = self.upsample(h)\n",
        "        h = self.convm1(h)\n",
        "        h = self.bn2(h)\n",
        "        h = self.relu(h)\n",
        "        h = self.convm2(h)\n",
        "\n",
        "        # compute skip\n",
        "        s = self.upsample(x)\n",
        "        s = self.sconv(s)\n",
        "\n",
        "        return h + s\n",
        "\n",
        "\n",
        "class Render(nn.Module):\n",
        "    def __init__(self, cin, colors=3):\n",
        "        \"\"\"Render an image given the parameters\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            colors (int, optional): Color channels. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(Render, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(cin)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv = nn.Conv2d(cin, colors, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator_CLSTM(nn.Module):\n",
        "    def __init__(self, z_dim=256,\n",
        "                 tempc=1024,\n",
        "                 zt_dim=3,\n",
        "                 upchannels=[512, 256, 128],\n",
        "                 subchannels=[64, 32, 32],\n",
        "                 n_frames=16,\n",
        "                 colors=3):\n",
        "        \"\"\"Full generator CLSTM model\n",
        "\n",
        "        Args:\n",
        "            z_dim (int, optional): Latent z. Defaults to 256.\n",
        "            tempc (int, optional): CLSTM channels. Defaults to 1024.\n",
        "            zt_dim (int, optional): CLSTM window size. Defaults to 3.\n",
        "            upchannels (list, optional): Defaults to [512, 256, 128].\n",
        "            subchannels (list, optional): Defaults to [64, 32, 32].\n",
        "            n_frames (int, optional): Frames to generate. Defaults to 16.\n",
        "            colors (int, optional): Number of colors. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(Generator_CLSTM, self).__init__()\n",
        "        assert len(subchannels) == 3\n",
        "        self.tempc = tempc\n",
        "        self.zt_dim = zt_dim\n",
        "        self.colors = colors\n",
        "\n",
        "        self.fc = nn.Linear(z_dim, zt_dim**2 * tempc)\n",
        "        self.temp = CLSTM(tempc, n_frames)\n",
        "\n",
        "        self.build = nn.Sequential()\n",
        "        for i in range(len(upchannels)):\n",
        "            if not i:\n",
        "                self.build.add_module('Up1', Up(tempc, upchannels[0]))\n",
        "            else:\n",
        "                self.build.add_module(f'Up{i+1}', Up(upchannels[i-1],\n",
        "                                      upchannels[i]))\n",
        "\n",
        "        self.buildr = Render(upchannels[-1], colors=colors)\n",
        "\n",
        "        self.sup1 = Up(upchannels[-1], subchannels[0])\n",
        "        self.sup1r = Render(subchannels[0], colors=colors)\n",
        "        self.sup2 = Up(subchannels[0], subchannels[1])\n",
        "        self.sup2r = Render(subchannels[1], colors=colors)\n",
        "        self.sup3 = Up(subchannels[1], subchannels[2])\n",
        "        self.sup3r = Render(subchannels[2], colors=colors)\n",
        "\n",
        "    def subsample(self, h, N, T, frames=4):\n",
        "        # to vid\n",
        "        _, C, H, W = h.shape\n",
        "        h = h.view(N, T, C, H, W)\n",
        "        h = h[:, np.random.randint(min(frames, T))::frames]\n",
        "        N, T, C, H, W = h.shape\n",
        "        # to train\n",
        "        h = h.contiguous().view(N * T, C, H, W)\n",
        "        return h, T\n",
        "\n",
        "    def forward(self, z, test=False):\n",
        "        \"\"\"Compute generator forward pass\n",
        "\n",
        "        Args:\n",
        "            z (torch.Tensor): Latent z [batch_size, z_dim]\n",
        "            test (bool, optional): Produce test videos. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            list(torch.Tensor) or torch.Tensor: Subsampled or regular videos\n",
        "        \"\"\"\n",
        "        h = self.fc(z)\n",
        "        h = h.view(-1, self.tempc, self.zt_dim, self.zt_dim)\n",
        "        h = self.temp(h)\n",
        "        N, T, C, H, W = h.shape\n",
        "        h = h.view(N*T, C, H, W)\n",
        "        h = self.build(h)\n",
        "\n",
        "        outsize = self.zt_dim * 2 ** (len(self.build) + 3)\n",
        "\n",
        "        if test:\n",
        "            h = self.sup1(h)\n",
        "            h = self.sup2(h)\n",
        "            h = self.sup3(h)\n",
        "            h = self.sup3r(h).view(N, T, self.colors, outsize,\n",
        "                                   outsize).transpose(1, 2)\n",
        "\n",
        "            return h\n",
        "        else:\n",
        "            # render 1st\n",
        "            x1 = self.buildr(h).view(N, T, self.colors, outsize // 8,\n",
        "                                     outsize // 8)\n",
        "            h, T = self.subsample(h, N, T)\n",
        "            h = self.sup1(h)\n",
        "            # render 2nd\n",
        "            x2 = self.sup1r(h).view(N, T, self.colors, outsize // 4,\n",
        "                                    outsize // 4)\n",
        "            h, T = self.subsample(h, N, T)\n",
        "            h = self.sup2(h)\n",
        "            # render 3rd\n",
        "            x3 = self.sup2r(h).view(N, T, self.colors, outsize // 2,\n",
        "                                    outsize // 2)\n",
        "            h, T = self.subsample(h, N, T)\n",
        "            h = self.sup3(h)\n",
        "            # render 4th\n",
        "            x4 = self.sup3r(h).view(N, T, self.colors, outsize, outsize)\n",
        "\n",
        "        return x1, x2, x3, x4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3XsqOEKqV0FC"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import errno\n",
        "import numpy as np\n",
        "import torch\n",
        "import codecs\n",
        "\n",
        "\n",
        "class MovingMNIST(data.Dataset):\n",
        "    \"\"\"`MovingMNIST <http://www.cs.toronto.edu/~nitish/unsupervised_video/>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where ``processed/training.pt``\n",
        "            and  ``processed/test.pt`` exist.\n",
        "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
        "            otherwise from ``test.pt``.\n",
        "        split (int, optional): Train/test split size. Number defines how many samples\n",
        "            belong to test set.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in an PIL\n",
        "            image and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "    \"\"\"\n",
        "    urls = [\n",
        "        'https://github.com/tychovdo/MovingMNIST/raw/master/mnist_test_seq.npy.gz'\n",
        "    ]\n",
        "    raw_folder = 'raw'\n",
        "    processed_folder = 'processed'\n",
        "    training_file = 'moving_mnist_train.pt'\n",
        "    test_file = 'moving_mnist_test.pt'\n",
        "\n",
        "    def __init__(self, root, train=True, split=1000, transform=None, target_transform=None, download=True):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.split = split\n",
        "        self.train = train  # training set or test set\n",
        "\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError('Dataset not found.' +\n",
        "                               ' You can use download=True to download it')\n",
        "\n",
        "        if self.train:\n",
        "            self.train_data = torch.load(\n",
        "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
        "        else:\n",
        "            self.test_data = torch.load(\n",
        "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (seq, target) where sampled sequences are splitted into a seq\n",
        "                    and target part\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            seq, target = self.train_data[index, :10], self.train_data[index, 10:]\n",
        "        else:\n",
        "            seq, target = self.test_data[index, :10], self.test_data[index, 10:]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        # seq = Image.fromarray(seq.numpy(), mode='L')\n",
        "        # target = Image.fromarray(target.numpy(), mode='L')\n",
        "\n",
        "        # if self.transform is not None:\n",
        "        #     seq = self.transform(seq)\n",
        "\n",
        "        # if self.target_transform is not None:\n",
        "        #     target = self.target_transform(target)\n",
        "\n",
        "        return seq, target\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        else:\n",
        "            return len(self.test_data)\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
        "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the Moving MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
        "        from six.moves import urllib\n",
        "        import gzip\n",
        "\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        # download files\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
        "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        for url in self.urls:\n",
        "            print('Downloading ' + url)\n",
        "            data = urllib.request.urlopen(url)\n",
        "            filename = url.rpartition('/')[2]\n",
        "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
        "                    gzip.GzipFile(file_path) as zip_f:\n",
        "                out_f.write(zip_f.read())\n",
        "            os.unlink(file_path)\n",
        "\n",
        "        # process and save as torch files\n",
        "        print('Processing...')\n",
        "\n",
        "        training_set = torch.from_numpy(\n",
        "            np.load(os.path.join(self.root, self.raw_folder, 'mnist_test_seq.npy')).swapaxes(0, 1)[:-self.split]\n",
        "        )\n",
        "        test_set = torch.from_numpy(\n",
        "            np.load(os.path.join(self.root, self.raw_folder, 'mnist_test_seq.npy')).swapaxes(0, 1)[-self.split:]\n",
        "        )\n",
        "\n",
        "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
        "            torch.save(training_set, f)\n",
        "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
        "            torch.save(test_set, f)\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        tmp = 'train' if self.train is True else 'test'\n",
        "        fmt_str += '    Train/test: {}\\n'.format(tmp)\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C4EariMFV535",
        "outputId": "2ebb1c36-f08e-459a-89ee-7471d032b919"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from skvideo import io\n",
        "from tqdm.gui import tqdm\n",
        "\n",
        "\n",
        "def genSamples(g, n=8, e=1):\n",
        "    with torch.no_grad():\n",
        "        s = g(torch.rand((n**2, 256), device='cuda')*2-1,\n",
        "              test=True).cpu().detach().numpy()\n",
        "    out = np.zeros((1, 20, 64*n, 64*n))\n",
        "\n",
        "    for j in range(n):\n",
        "        for k in range(n):\n",
        "            out[:, :, 64*j:64*(j+1), 64*k:64*(k+1)] = s[j*n + k, 0, :, :, :]\n",
        "\n",
        "    out = out.transpose((1, 2, 3, 0))\n",
        "    out = (np.concatenate([out, out, out], axis=3)+1) / 2 * 255\n",
        "    io.vwrite(f'tganv2moving/gensamples_id{e}.gif', out)\n",
        "\n",
        "\n",
        "def subsample_real(h, frames=4):\n",
        "    h = h[:, np.random.randint(min(frames, h.shape[1]))::frames]\n",
        "    return h\n",
        "\n",
        "\n",
        "def full_subsample_real(h, frames=4):\n",
        "    out = []\n",
        "    for i in range(4):\n",
        "        if i:\n",
        "            out.append(subsample_real(out[i-1], frames=frames))\n",
        "        else:\n",
        "            out.append(h)\n",
        "\n",
        "    for i in range(4):\n",
        "        for j in range(3-i):\n",
        "            out[i] = F.avg_pool3d(out[i], kernel_size=(1, 2, 2))\n",
        "    return out\n",
        "\n",
        "\n",
        "def zero_centered_gp(real_data, pr):\n",
        "    gradients = torch.autograd.grad(outputs=pr, inputs=real_data,\n",
        "                                    grad_outputs=torch.ones_like(pr),\n",
        "                                    create_graph=True, retain_graph=True)\n",
        "\n",
        "    return sum([torch.sum(torch.square(g)) for g in gradients])\n",
        "\n",
        "\n",
        "def train():\n",
        "    epochs = 10000\n",
        "    batch_size = 32\n",
        "    lambda_val = 0.5\n",
        "\n",
        "    # data\n",
        "    test = MovingMNIST('moving/', train=False)\n",
        "    loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=True,\n",
        "                                         drop_last=True)\n",
        "\n",
        "    def dataGen():\n",
        "        while True:\n",
        "            for d in loader:\n",
        "                yield d\n",
        "\n",
        "    dg = dataGen()\n",
        "    # gen model\n",
        "    dis = DisMultiResNet(channels=[32, 32, 64, 128, 256], colors=1).cuda()\n",
        "    gen = Generator_CLSTM(\n",
        "        tempc=256,\n",
        "        zt_dim=4,\n",
        "        upchannels=[128],\n",
        "        subchannels=[64, 32, 32],\n",
        "        n_frames=20,\n",
        "        colors=1\n",
        "    ).cuda()\n",
        "    # original paper doesn't use TTUR but it is generally effective\n",
        "    disOpt = torch.optim.Adam(dis.parameters(), lr=5e-5, betas=(0, 0.9))\n",
        "    genOpt = torch.optim.Adam(gen.parameters(), lr=1e-4, betas=(0, 0.9))\n",
        "\n",
        "    # train\n",
        "    # note on loss function: within the current github repo they\n",
        "    # employ softplus linear loss, if the normal cross entropy\n",
        "    # is desired one may simply change the comments\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # discriminator\n",
        "        disOpt.zero_grad()\n",
        "        real = torch.cat(next(dg), dim=1).cuda().unsqueeze(2)\n",
        "        real = real.to(dtype=torch.float32) / 255 * 2 - 1\n",
        "        real = full_subsample_real(real)\n",
        "        for i in real:\n",
        "            i.requires_grad = True\n",
        "        pr = dis(real)\n",
        "        dis_loss = zero_centered_gp(real, pr) * lambda_val\n",
        "        with torch.no_grad():\n",
        "            fake = gen(torch.rand((batch_size, 256), device='cuda')*2-1)\n",
        "        pf = dis(fake)\n",
        "        # dis_loss = -torch.mean(torch.log(pr) + torch.log(1-pf))\n",
        "        dis_loss += torch.mean(F.softplus(-pr)) + torch.mean(F.softplus(pf))\n",
        "        dis_loss.backward()\n",
        "        disOpt.step()\n",
        "        # generator\n",
        "        genOpt.zero_grad()\n",
        "        fake = gen(torch.rand((batch_size, 256), device='cuda')*2-1)\n",
        "        pf = dis(fake)\n",
        "        # gen_loss = -torch.mean(torch.log(pf))\n",
        "        gen_loss = torch.mean(F.softplus(-pf))\n",
        "        gen_loss.backward()\n",
        "        genOpt.step()\n",
        "        # log results\n",
        "        print('Epoch', epoch, 'Dis', dis_loss.item(), 'Gen', gen_loss.item())\n",
        "        if epoch % 100 == 0:\n",
        "            genSamples(gen, e=epoch)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Sx7y0VXOlO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
